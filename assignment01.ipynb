{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9iIbU1S_Yug"
   },
   "source": [
    "HHU Deep Learning in Life Science: Generative Models, Prof. Dr. **Markus Kollmann**, WS 2025-2026\n",
    "\n",
    "Tutors: **Felix Michels** and **Adaloglou Nikolaos**\n",
    "## Assignment 1 - Variational Autoencoder (VAE)\n",
    "\n",
    "- **Submission link**: https://uni-duesseldorf.sciebo.de/s/ZSjNF5GWWzHWcmf \n",
    "- **Deadline**: 30/11/2025\n",
    "- **Note**: Rename your file BEFORE submitting!\n",
    "- If you encounter issues that might apply to other students, please notify others in the group chat.\n",
    "\n",
    "#### How to submit\n",
    "- Submit the solved notebook (not a zip) with your full name (`NAME_SURNAME`) plus assingment number for the filename as an indicator, e.g `max_mustermann_a1.ipynb` for `assignment 1`. If you have more names, please use only the first one for all homeworks.\n",
    "- Please use the same naming convention `NAME_SURNAME_aX` to faciliate the tutors.\n",
    "- You are free to modify our code as you see fit.\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## General instructions\n",
    "---\n",
    "Instructions about the exercises and the course structure.\n",
    "\n",
    "**Course Structure & Grading:**\n",
    "\n",
    "-  A weekly/biweekly exercise is uploaded after the lecture, and you have 1 or 2 weeks to complete it. Your solution will be graded as PASS/FAIL. \n",
    "- You need 80% of exercises completed to be eligible for the final written exam\n",
    "- Expected results are provided as guidance, but exact matches aren't always required \n",
    "- Typically, we provide you with the expected results (e.g., 90% test classification accuracy), but you don't always need to match the expected result to get the point.\n",
    "- Importantly, your code should produce the last result, based on which you will be graded.\n",
    "- We will not rerun your notebooks; your submission will be evaluated based on the cell's output.\n",
    "\n",
    "The exercises are not going to affect your final grade, it's simply an admission to participate in the final exam. For this reason, AI tools and chatbots are *not* prohibited. You are responsible for your code and its outcome.\n",
    "\n",
    "**Submission Policy:**\n",
    "\n",
    "You are incentivized to ask questions and, of course, help each other to some extent through Rocketchat. Please use threads to resolve different issues/questions. We will try to catch up and answer the questions as much as our agendas enable us. If you are sick or don't submit for some personal reason, we will not give you an extension. Please don't spam us for individual extensions; it's unfair to the other students who submit by the specified deadlines. Extensions are only given in extreme cases to all students, such as if Sciebo is down for a few days, etc. \n",
    "\n",
    "\n",
    "\n",
    "If you have issues with the grading procedure, or administrative problems with the course, grading, etc., contact Prof. Kollmann. The tutors are not responsible for the administrative aspects of the course.\n",
    "\n",
    "Feel free to ask us for any information or questions about the course during the tutoring classes (after the lecture). The tutoring class is not just for us to show you the solution. It can become more interactive if you participate and ask questions. If you don't have any questions, we assume you understand everything, and we will only be able to present the solution to the current exercise.\n",
    "\n",
    "**AI Tools & Collaboration:**\n",
    "- AI tools and chatbots are permitted (you're responsible for understanding your code)\n",
    "- Collaboration through Rocketchat is encouraged - use threads for different topics\n",
    "\n",
    "\n",
    "**Tutoring Sessions:**\n",
    "- Attend tutoring classes after lectures for interactive Q&A\n",
    "- Come prepared with questions to make sessions more valuable\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nM59aqEd_Yuj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZlTDUOG_Yul",
    "outputId": "594bd1e0-8000-4567-c4d0-10690877132d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "outdir = Path('student_figures')\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f38hiycl_Yum"
   },
   "source": [
    "# Part A: VAEs in 2D Toy Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5meG4owb_Yum"
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Jiw4XeI_Yum"
   },
   "outputs": [],
   "source": [
    "def sample_data_1():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    return [[1.0, 2.0]] + rand.randn(count, 2) * [[5.0, 1.0]]\n",
    "\n",
    "def sample_data_2():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    return [[1.0, 2.0]] + (rand.randn(count, 2) * [[5.0, 1.0]]) \\\n",
    "    @ ([[np.sqrt(2)/2, np.sqrt(2)/2],  \\\n",
    "      [-np.sqrt(2)/2, np.sqrt(2)/2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FH_QxoJ_Yun"
   },
   "outputs": [],
   "source": [
    "X1 = sample_data_1()\n",
    "N_test1 = 5000\n",
    "N_val1 = 5000\n",
    "N_train1 = X1.shape[0] - N_val1 - N_test1\n",
    "device = torch.device('cuda')\n",
    "train_dataset1 = TensorDataset(torch.as_tensor(X1[:N_train1], dtype=torch.float32, device=device))\n",
    "val_dataset1 = TensorDataset(torch.as_tensor(X1[N_train1:-N_test1], dtype=torch.float32, device=device))\n",
    "test_dataset1 = TensorDataset(torch.as_tensor(X1[-N_test1:], dtype=torch.float32, device=device))\n",
    "\n",
    "X2 = sample_data_2()\n",
    "N_test2 = 5000\n",
    "N_val2 = 5000\n",
    "N_train2 = X2.shape[0] - N_val2 - N_test2\n",
    "train_dataset2 = TensorDataset(torch.as_tensor(X2[:N_train2], dtype=torch.float32, device=device))\n",
    "val_dataset2 = TensorDataset(torch.as_tensor(X2[N_train2:-N_test2], dtype=torch.float32, device=device))\n",
    "test_dataset2 = TensorDataset(torch.as_tensor(X2[-N_test2:], dtype=torch.float32, device=device))\n",
    "\n",
    "plt.title('Dataset 1')\n",
    "plt.scatter(X1[:, 0], X1[:, 1], s=2);\n",
    "plt.figure()\n",
    "plt.title('Dataset 2')\n",
    "plt.scatter(X2[:, 0], X2[:, 1], s=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xojQYO2V_Yup"
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0ABRFt0_Yup"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, hidden_activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_in, layer_out in zip([input_size] + hidden_sizes, hidden_sizes + [output_size]):\n",
    "            self.layers.append(nn.Linear(layer_in, layer_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.hidden_activation(x)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A1: Implement the VAE\n",
    "\n",
    "- The following class methods are optional to implement and serve as a guideline. Feel free to write your own implementation.\n",
    "- In part B, we use the same implementation, where flatten and sigmoid will be instansiated with True.\n",
    "**Objective:** Complete the VAE implementation with encoder-decoder architecture and reparameterization trick.\n",
    "\n",
    "**Key Requirements:**\n",
    "- Implement the `forward()` method with proper loss calculations\n",
    "- The encoder and decoder neural nets output mean and log-variance for their respective distribution\n",
    "- Use the reparameterization trick for sampling from the posterior\n",
    "- Calculate KL divergence and negative log-likelihood losses\n",
    "- Return reconstruction, latent sample, ELBO, KL loss, and NLL loss\n",
    "- Implement the `sample()` method.\n",
    "\n",
    "**Implementation Notes:**\n",
    "- For Part B (MNIST), set `flatten=True` and `sigmoid=True`\n",
    "- The provided method signatures are optional - feel free to use your own approach\n",
    "- Ensure proper gradient flow through the reparameterization trick\n",
    "\n",
    "**Mathematical Background:**\n",
    "- Posterior: q_φ(z|x) = N(z; μ_φ(x), σ²_φ(x))\n",
    "- Prior: p(z) = N(z; 0, I)\n",
    "- Decoder: p_θ(x|z) = N(x; μ_θ(z), σ²_θ(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PyrRMYm_Yuq"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size, encoder_sizes, decoder_sizes, hidden_activation=F.relu,\n",
    "                 flatten=False, sigmoid=False):\n",
    "        \"\"\"Variational Autoencoder (VAE) model.\n",
    "        Args:\n",
    "            input_size (int): _dimension of input data_\n",
    "            latent_size (int): _dimension of latent variable_\n",
    "            encoder_sizes (list): _sizes of hidden layers in encoder MLP_\n",
    "            decoder_sizes (list): _sizes of hidden layers in decoder MLP_\n",
    "            hidden_activation (callable, optional): _activation function for hidden layers_. Defaults to F.relu.\n",
    "            flatten (bool, optional): _if True, flattens the input_. Defaults to False.\n",
    "            sigmoid (bool, optional): _if True, applies sigmoid activation to the output_. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.flatten = flatten\n",
    "        self.sigmoid = sigmoid\n",
    "        # The output is 2 * latent_size to output both mean and log-variance\n",
    "        # for both encoder and decoder\n",
    "        self.encoder = MLP(input_size, encoder_sizes, 2 * latent_size, hidden_activation)\n",
    "        self.decoder = MLP(latent_size, decoder_sizes, 2 * input_size, hidden_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Return values:\n",
    "        # x_pred : reconstruction of input x\n",
    "        # z: latent variable sampled from posterior distribution p(z|x)\n",
    "        # nll: negative log likelihood loss = mse for Gaussian distribution\n",
    "        # kl: Kullback-Leibler-divergence for multivariate normal distributions\n",
    "        # ELBO: evidence lower bound\n",
    "        return x_pred, z, elbo, kl, nll\n",
    "\n",
    "    def encode(self, x):\n",
    "        # No need to use this class method, this is just our implementation\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def decode(self, z, noise=True):\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "\n",
    "        return x_pred # shape: [batch_size, input_size]\n",
    "\n",
    "    def sample(self, batch_size, decoder_noise=False):\n",
    "        \"\"\"\n",
    "        Samples from the generative model p(x) = ∫p(x|z)p(z) dz\n",
    "        Args:\n",
    "            batch_size (int): number of samples to generate\n",
    "            decoder_noise (bool, optional): if True, samples from p(x|z), otherwise uses the mean of p(x|z). Defaults to False.\n",
    "        Returns:\n",
    "            x_sampled (torch.Tensor): samples from the generative model of shape [batch_size, input_size]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            ### START CODE HERE ###\n",
    "            ### END CODE HERE ###\n",
    "            return x_sampled\n",
    "\n",
    "    def sample_prior(self, batch_size, device=None):\n",
    "        \"\"\"Samples from the prior p(z) = N(0, I)\"\"\"\n",
    "        if device is None:\n",
    "            device = next(self.parameters()).device\n",
    "        z = torch.randn(batch_size, self.latent_size, device=device)\n",
    "        return z # [bs, latent_size]\n",
    "\n",
    "    def sample_posterior(self, mean, std):\n",
    "        \"\"\"this implements the reparameterization trick\"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "        return z\n",
    "\n",
    "    def encode_latent(self, x):\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "        z = self.sample_posterior(mean, std)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fwoi3XLP_Yur"
   },
   "source": [
    "### Task A2: Implement the training logic\n",
    "\n",
    "\n",
    "\n",
    "**Objective:** Complete the training loop for Beta-VAE with adjustable KL weighting.\n",
    "\n",
    "**Key Requirements:**\n",
    "- Implement the forward pass and loss computation for each mini-batch. Implement the loss based on the VAE implementation.\n",
    "- Use the Beta-VAE objective: Loss = MSE + β × KL_divergence\n",
    "- For Part A, use β = 1 (standard VAE)\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$ L = \\text{NLL} + \\beta \\cdot \\text{KL} = -\\log p_\\theta(x|z) + \\beta \\cdot D_{KL}(q_\\phi(z|x) || p(z)) $$\n",
    "- MSE (mean square error) is the negative log likelihood given a gaussian distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU_hKaUQ_Yur"
   },
   "outputs": [],
   "source": [
    "def train_vae(model, optimizer, train_loader, val_loader, test_loader=None, figtitle=None, num_epochs=10, beta=1.0):\n",
    "    train_elbo_history = []\n",
    "    train_kl_history = []\n",
    "    train_nll_history = []\n",
    "\n",
    "    val_elbo_history = []\n",
    "    val_kl_history = []\n",
    "    val_nll_history = []\n",
    "    device = next(model.parameters()).device\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for minibatch in train_loader:\n",
    "\n",
    "            # elbo, kl, nll must be computed for each minibatch\n",
    "            ### START CODE HERE ###\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            train_elbo_history.append(elbo.item())\n",
    "            train_kl_history.append(kl.item())\n",
    "            train_nll_history.append(nll.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            elbo, kl, nll = zip(*[tuple(t.item() for t in model(minibatch[0].to(device))[2:]) for minibatch in val_loader])\n",
    "            val_elbo_history.append(np.mean(elbo))\n",
    "            val_kl_history.append(np.mean(kl))\n",
    "            val_nll_history.append(np.mean(nll))\n",
    "\n",
    "    if test_loader is not None:\n",
    "        with torch.no_grad():\n",
    "            elbo, kl, nll = zip(*[tuple(t.item() for t in model(minibatch[0].to(device))[2:]) for minibatch in test_loader])\n",
    "            test_elbo = np.mean(elbo)\n",
    "            test_kl = np.mean(kl)\n",
    "            test_nll = np.mean(nll)\n",
    "\n",
    "\n",
    "    num_plots = 3\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(16,3))\n",
    "    if figtitle is not None:\n",
    "        fig.suptitle(figtitle)\n",
    "\n",
    "    x_plot_train = np.arange(len(train_elbo_history)) / len(train_loader)\n",
    "    x_plot_val = np.arange(num_epochs) + 1\n",
    "\n",
    "    axes[0].set_title('ELBO')\n",
    "    axes[0].plot(x_plot_train, train_elbo_history, label='train', zorder=1)\n",
    "    axes[0].scatter(x_plot_val, val_elbo_history, color='C1', label='val', zorder=2)\n",
    "    if test_loader is not None:\n",
    "        axes[0].axhline(test_elbo, color='C2', label='test', zorder=0)\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].set_title('KL divergence')\n",
    "    axes[1].plot(x_plot_train, train_kl_history, label='train', zorder=1)\n",
    "    axes[1].scatter(x_plot_val, val_kl_history, color='C1', label='val', zorder=2)\n",
    "    if test_loader is not None:\n",
    "        axes[1].axhline(test_kl, color='C2', label='test', zorder=0)\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].set_title('Negative log likelihood')\n",
    "    axes[2].plot(x_plot_train, train_nll_history, label='train', zorder=1)\n",
    "    axes[2].scatter(x_plot_val, val_nll_history, color='C1', label='val', zorder=2)\n",
    "    if test_loader is not None:\n",
    "        axes[2].axhline(test_nll, color='C2', label='test', zorder=0)\n",
    "    axes[2].legend()\n",
    "\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "\n",
    "    fig2 = None\n",
    "    if test_loader:\n",
    "        fig2, axes = plt.subplots(1, 3, figsize=(16,3), sharex=True, sharey=True)\n",
    "        original_data = torch.cat([minibatch[0] for minibatch in test_loader], dim=0).cpu().numpy()\n",
    "        axes[0].set_title('Original data')\n",
    "        axes[0].scatter(original_data[:, 0], original_data[:, 1], s=2)\n",
    "\n",
    "        samples = model.sample(5000, decoder_noise=True).cpu().numpy()\n",
    "        axes[1].set_title('Samples (with decoder noise)')\n",
    "        axes[1].scatter(samples[:, 0], samples[:, 1], s=2)\n",
    "\n",
    "        samples = model.sample(5000, decoder_noise=False).cpu().numpy()\n",
    "        axes[2].set_title('Samples (without decoder noise)')\n",
    "        axes[2].scatter(samples[:, 0], samples[:, 1], s=2)\n",
    "\n",
    "        print(f'Final test performance: ELBO = {test_elbo:.5f}, KL = {test_kl:.5f}, NLL = {test_nll:.5f}')\n",
    "    return fig, fig2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd3me1Dd_Yus"
   },
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "Variational autoencoder with\n",
    "- prior p(z) = N(z; 0, I).\n",
    "- posterior q_θ(z|x) = N(z; µ_θ(x), Σ_θ(x))) with diagonal covariance matrix\n",
    "- decoder p_θ(x|z) = N(x; µ_θ(z), Σ_θ(z)) with diagonal covariance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "FEkrOKlJ_Yut",
    "outputId": "8e39da59-6d07-496e-fbae-b524ebc62c76"
   },
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "latent_size = 2\n",
    "encoder_sizes = [20]\n",
    "decoder_sizes = []\n",
    "hidden_activation = F.relu\n",
    "lr = 1e-2\n",
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "train_loader = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset1, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
    "model = VAE(input_size, latent_size, encoder_sizes, decoder_sizes, hidden_activation).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "fig, fig2 = train_vae(model, optimizer, train_loader, val_loader, test_loader, figtitle=f'Dataset 1, VAE 1')\n",
    "# save both figures\n",
    "fig.savefig(outdir / 'part_a_vae_dataset1.png', bbox_inches='tight')\n",
    "fig2.savefig(outdir / 'part_a_vae_dataset1_samples.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiLbx6ck_Yut"
   },
   "source": [
    "#### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "rZcT4Qi__Yuu",
    "outputId": "bb695492-bcd3-4565-9fc6-2129f4761c8b"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset2, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n",
    "model = VAE(input_size, latent_size, encoder_sizes, decoder_sizes, hidden_activation).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "fig, fig2 = train_vae(model, optimizer, train_loader, val_loader, test_loader, figtitle=f'Dataset 2, VAE 1')\n",
    "fig.savefig(outdir / 'part_a_vae_dataset2.png', bbox_inches='tight')\n",
    "fig2.savefig(outdir / 'part_a_vae_dataset2_samples.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmEj9wsL_Yuv"
   },
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR10fS1c_Yuv"
   },
   "outputs": [],
   "source": [
    "def sample_data_3():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    a = [[-1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
    "    b = [[1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
    "    c = np.c_[2 * np.cos(np.linspace(0, np.pi, count // 3)),\n",
    "    -np.sin(np.linspace(0, np.pi, count // 3))]\n",
    "    c += rand.randn(*c.shape) * 0.2\n",
    "    data_x = np.concatenate([a, b, c], axis=0)\n",
    "    data_y = np.array([0] * len(a) + [1] * len(b) + [2] * len(c))\n",
    "    perm = rand.permutation(len(data_x))\n",
    "    return data_x[perm], data_y[perm]\n",
    "\n",
    "X3, y3 = sample_data_3()\n",
    "N_test3 = 20000\n",
    "N_val3 = 5000\n",
    "N_train3 = X3.shape[0] - N_val3 - N_test3\n",
    "train_dataset3 = TensorDataset(torch.as_tensor(X3[:N_train3], dtype=torch.float32, device=device))\n",
    "val_dataset3 = TensorDataset(torch.as_tensor(X3[N_train3:-N_test3], dtype=torch.float32, device=device))\n",
    "test_dataset3 = TensorDataset(torch.as_tensor(X3[-N_test3:], dtype=torch.float32, device=device))\n",
    "plt.title('Dataset 3')\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=y3, s=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en3TSBio_Yuv"
   },
   "source": [
    "### Training for Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNglSla9_Yuv"
   },
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "latent_size = 2\n",
    "encoder_sizes = [150, 250] # two hidden layers\n",
    "decoder_sizes = [250, 150] # two hidden layers\n",
    "lr = 1e-4\n",
    "batch_size = 500\n",
    "num_epochs = 10\n",
    "train_loader = DataLoader(train_dataset3, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset3, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset3, batch_size=batch_size, shuffle=False)\n",
    "model = VAE(input_size, latent_size, encoder_sizes, decoder_sizes, hidden_activation).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "fig, fig2 = train_vae(model, optimizer, train_loader, val_loader, test_loader, figtitle=f'Dataset 3, VAE 1')\n",
    "fig.savefig(outdir / 'part_a_vae_dataset3.png', bbox_inches='tight')\n",
    "fig2.savefig(outdir / 'part_a_vae_dataset3_samples.png', bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Latent Space Representation')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "with torch.no_grad():\n",
    "    z = model.encode_latent(torch.as_tensor(X3[:500], dtype=torch.float32, device=device)).cpu().numpy()\n",
    "    plt.scatter(z[:, 0], z[:, 1], c=y3[:500])\n",
    "plt.savefig(outdir / 'part_a_vae_dataset3_latent.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B - vanilla VAE and Beta VAE on images\n",
    "\n",
    "This part is about implementing an Autoencoder, a vanilla VAE and a Beta VAE on the MNIST dataset of handwritten digits and exploring their characteristics with regards to the latent space.\n",
    "\n",
    "\n",
    "Let's start by defining the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST('./data',\n",
    "               transform=torchvision.transforms.ToTensor(),\n",
    "               download=True),\n",
    "        batch_size=512,\n",
    "        shuffle=True)\n",
    "\n",
    "val_data = torch.utils.data.DataLoader(\n",
    "              torchvision.datasets.MNIST('./data',\n",
    "                     transform=torchvision.transforms.ToTensor(),\n",
    "                     train=False,\n",
    "                     download=False),\n",
    "              batch_size=1024,\n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B1: Autoencoder Implementation\n",
    "\n",
    "**Objective:** Implement a standard autoencoder for comparison with VAE variants.\n",
    "\n",
    "**Key Requirements:**\n",
    "- Complete the `forward()` methods in `Encoder` and `Decoder` classes\n",
    "- Implement the training function with MSE reconstruction loss\n",
    "- Ensure proper tensor reshaping: flatten input (28×28 → 784) and reshape output (784 → 28×28)\n",
    "- Loss function: MSE between input and reconstruction. No regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 512)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(512, latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, 512)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(512, 784)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        ### START CODE HERE ###\n",
    "        ### END CODE HERE ###\n",
    "        # Note: reshape to image dimensions\n",
    "        return out\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z) # includes sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z), z\n",
    "\n",
    "def train_ae(autoencoder, data, opt, num_epochs=20):\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in data:\n",
    "            ### START CODE HERE ###\n",
    "            ### END CODE HERE ###\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B2: Train Multiple Model Variants\n",
    "\n",
    "**Objective:** Train and compare autoencoder with VAEs using different β values.\n",
    "\n",
    "**Models to Train:**\n",
    "1. **Standard Autoencoder** (no regularization)\n",
    "2. **VAE with β=1** (standard VAE)\n",
    "3. **VAE with β=3** (moderate disentanglement)\n",
    "4. **VAE with β=10** (high disentanglement)\n",
    "\n",
    "**Training Configuration:**\n",
    "- Epochs: 15-20\n",
    "- Latent dimension: 2 (enables direct 2D visualization)\n",
    "\n",
    "- **Reccomended**: Save all VAEs in a dictionary with their beta as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "input_size = 28*28\n",
    "latent_size = 2\n",
    "encoder_sizes = [512]\n",
    "decoder_sizes = [512]\n",
    "num_epochs = 20\n",
    "\n",
    "### START CODE HERE ###\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B3:  Visualize the latent space\n",
    "\n",
    "**Objective:** Compare latent space organization across all four model variants using 2D scatter plots.\n",
    "\n",
    "**Visualization Requirements:**\n",
    "- Create scatter plots for AE and VAE models (β=[1,3,10])\n",
    "- Use MNIST digit labels (0-9) for color coding\n",
    "- Sample >1500 random test images for clear visualization\n",
    "- Plot latent encodings directly (no dimensionality reduction needed since latent_dim=2)\n",
    "\n",
    "**Expected Observations:**\n",
    "- **Autoencoder:** May show overlapping clusters\n",
    "- **VAE (β=3,10):** Better separated clusters, more organized structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent2D(model, data, num_batches=8, title=None, plot_idx=0):\n",
    "    plt.subplot(1,4,plot_idx+1)\n",
    "    ### START CODE HERE ###\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "# sample a subset of the test dataset randomly for better visualization\n",
    "### START CODE HERE ###\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task B4: Latent Space Sampling and Reconstruction\n",
    "\n",
    "**Objective:** Explore the learned latent space by sampling from a uniform grid and visualizing reconstructions.\n",
    "\n",
    "**Grid Sampling Setup:**\n",
    "- **Autoencoder:** Sample from grid x,y ∈ [-5,5] (12×12 points)\n",
    "- **VAEs:** Sample from grid x,y ∈ [-3,3] (12×12 points)\n",
    "- Use `np.linspace()` to create evenly spaced grid points\n",
    "\n",
    "**Visualization:**\n",
    "- Decode each grid point to generate 28×28 images\n",
    "- Arrange reconstructions in a 12×12 grid layout\n",
    "- Create one visualization per model (4 total)\n",
    "\n",
    "\n",
    "Randomly sample the latent space of the models and visualize the reconstructions. Sample latent vectors $z$ from the latent space over a uniform grid and plot the reconstructions on a grid. \n",
    "\n",
    "Hint: Use `np.linspace` with 2 loops (optional implementation).\n",
    "\n",
    "You should be able to observe that the reconstructed latent vectors look like digits, and the kind of digit corresponds to the location of the latent vector in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed(model, r0=(-5, 5), r1=(-5, 5), vae=False, n=12, title=None, plot_idx=0):\n",
    "    w=28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            ### START CODE HERE ###\n",
    "            ### END CODE HERE ###\n",
    "    plt.subplot(1,4,plot_idx+1)\n",
    "    plt.imshow(img, extent=[*r0, *r1])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(24,5))\n",
    "### START CODE HERE ###\n",
    "### END CODE HERE ###\n",
    "plt.savefig(outdir / 'part_b_reconstructions.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B5: Latent Space Interpolation\n",
    "\n",
    "**Objective:** Demonstrate smooth interpolation between different digits in latent space. Given two inputs $x_1$ and $x_2$, and their corresponding latent vectors $z_1$ and $z_2$, interpolate between them by decoding latent vectors between $x_1$ and $x_2$. \n",
    "\n",
    "**Procedure:**\n",
    "1. Select one random digit with label 0 and one with label 1\n",
    "2. Encode both images to get latent vectors z₁ and z₂\n",
    "3. Create interpolation path: z(t) = z₁ + (z₂ - z₁) × t, where t ∈ [0,1]\n",
    "4. Use n=15 equally spaced values of t\n",
    "5. Reconstruct the images for the interpolated values and plot them in a single row.\n",
    "\n",
    "Do the aforementioned for the all 4 model configurations.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "z_interpolated = z₁ + (z₂ - z₁) × t, where t = [0, 1/14, 2/14, ..., 1]\n",
    "\n",
    "**Implementation for All Models:**\n",
    "- Repeat interpolation for autoencoder and all three VAE variants\n",
    "- Display results in 4 rows (one per model)\n",
    "- Compare smoothness and quality of transitions\n",
    "\n",
    "**Expected Observations:**\n",
    "- VAEs typically show smoother, more meaningful interpolations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(model, x_1, x_2, n=20, vae=False, title=None, plot_idx=0):\n",
    "    w = 28\n",
    "    img = np.zeros((w, n*w))\n",
    "    ### START CODE HERE ###\n",
    "    ### END CODE HERE ###\n",
    "    plt.subplot(4,1,plot_idx+1)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "fig, axes = plt.subplots(4,1, figsize=(27,8))\n",
    "\n",
    "# find a sample with label 1 and 0, with variavble names x_1, x_2\n",
    "### START CODE HERE ###\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.savefig(outdir / 'part_b_interpolations.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "solution_exercise3_vae.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
